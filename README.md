# Knowledge Distillation for Medical Image Classification

This repository implements **knowledge distillation** for medical image classification tasks using three datasets: Brain Tumor MRI, Medical MNIST, and Pneumonia Chest X-rays. Knowledge distillation trains a smaller, faster model (student) to mimic a larger, more accurate model (teacher), enabling deployment in resource-constrained environments without significant accuracy loss.

## Datasets

1. **Brain Tumor Detection**
   - **Task**: Binary classification (tumor vs. no tumor) using MRI images.

2. **Medical MNIST**
   - **Task**: Multi-class classification of medical images into six categories.

3. **Pneumonia Detection**
   - **Task**: Binary classification of healthy vs. pneumonia-affected lungs using chest X-rays.

## Approach

1. **Teacher Model**
   - A large, high-accuracy model is trained on each dataset to serve as the baseline.

2. **Student Model**
   - A smaller, lightweight model is trained using both:
     - Hard labels from the dataset.
     - Soft labels (probabilities) generated by the teacher model.

3. **Distillation Loss**
   - Combines the standard classification loss with a loss to match the teacherâ€™s predictions (soft labels).

## Dependencies

Ensure you have the following dependencies installed:

- Python 3.7+
- TensorFlow
- NumPy
- Matplotlib
- Pandas
- scikit-learn


